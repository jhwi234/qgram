Joe Notes 2024-02-02

Task 1:
For the Split type B do an after the fact analysis seeing which words weren't in the trinaing set and see how well it did for predicting the missing letter in them. Split type B was the version of the code were the frequcny of each token was maintained in the testing and training but the out of vocabulary nature of the training and test split wasn't maintained. So both training and testing tokens can contain the same word types. So what we're looking for is just finding out of the tokens that did uniquely occur in the testing set how well did the model perform on them. 

Task 2:
Exclude hapax legomena (words occurring only once within the dataset) from the training dataset, and instead, train the model on the remaining data. Following training, evaluate the model's performance on the hapax legomena. This approach mirrors Task 1 but with a distinct separation: the training data excludes hapaxes, which are then used as the testing data. The goal is to determine the model's proficiency in predicting missing letters in words that are hapax legomena, thus providing insight into its ability to handle rare or unique words.

Task 3:

Measure the informational content of the entire word list by calculating the entropy of each letter in every word. This involves estimating the uncertainty associated with each letter's occurrence in different contexts, such as when a letter is missing from a word. The process can be broken down into several steps:

Start by assigning a probability to every alphabet letter based on how often it appears in the language. Maximum entropy because of maximum uncertainity. This initial step helps us estimate how likely each letter is to occur in any given position.

Refine these probabilities with qgram analysis, which considers the specific letters surrounding a missing letter to improve our prediction accuracy. This process narrows down the most likely letters by comparing broader frequencybased estimates to those informed by immediate letter context, thus aiming to reduce our prediction's entropy or uncertainty.

Calculate entropy in two ways:
First, by assuming each letter is equally likely (uniform probability), representing the highest entropy scenario or the maximum uncertainty.

Second, using the letter frequencies (unigram probabilities) to see how much uncertainty there is without considering the surrounding letters.

Assess the information gain by comparing the entropy from our unigram model to that of the uniform probability model. The difference tells us how much more accurate our predictions are when using the qgram model over a random guess.

Finally, average the entropy across all words for each letter in different contexts to determine the overall informativeness of the qgram models. This means calculating the entropy for each scenario where a letter is missing, then averaging these values to get a sense of how well the qgram models perform across the entire corpus.

In essence, we're evaluating how effective qgram models are by starting with a broad prediction based on letter frequency, refining it with contextspecific data\*\*, and then measuring the improvement in our predictions through entropy reduction. This process highlights the value of context in improving the accuracy of our language models.

We need to get the overall information metric for the entire word list of the corpus. We can do this by getting the entropy for each letter in each word and in order to charactrerize how informative these qgram modles are and then do this for each word in the corpus and then average.

The process can be broken down into several steps:

 1. Initial Probability Assignment
 Objective: Estimate the likelihood of each letter's occurrence across all positions within a word, based on its overall frequency in the language. This foundational step assigns a baseline probability, \(P(l)\), to each letter \(l\) in the alphabet.
 Methodology: Compute the frequency of each letter in a large corpus, normalizing these frequencies to probabilities. This calculation forms the basis for further refinement and adjustment.

 2. Refinement Using QGram Probabilities
 Objective: Enhance the predictive accuracy of initial probabilities by incorporating the context provided by qgrams, which are sequences of characters including the missing letter and its immediate neighbors.
 Methodology: Adjust letter probabilities (\(P(l)\)) to \(P(l | \text{"context"})\) by analyzing qgram frequencies within the corpus. This step leverages conditional probabilities to account for the influence of surrounding letters, significantly reducing prediction entropy.

 3. Entropy Calculation
 Objective: Measure the uncertainty or information content associated with the occurrence of each letter in a given context, employing two distinct entropy calculations to evaluate this uncertainty.
 Methodology:
   Unigram Entropy (\(H_{\text{unigram}}(l)\)): Calculated from the refined probabilities, indicating the entropy without specific context.
   Uniform Entropy (\(H_{\text{uniform}}\)): Assumes an equal probability for all letters, serving as a baseline for maximum uncertainty.

 4. Information Gain (Reduction in Entropy)
 Objective: Quantify the improvement in predictive accuracy by comparing the entropy from the refined model against the uniform model, thereby illustrating the effectiveness of qgram analysis.
 Methodology: Determine the information gain by calculating the difference between \(H_{\text{unigram}}(l)\) and \(H_{\text{uniform}}\), with significant reductions indicating a substantial increase in predictive power due to contextual considerations.

 5. Average Entropy Across Words
 Objective: Evaluate the overall performance and informativeness of qgram models across the corpus by calculating the average entropy for each letter in different contexts.
 Methodology: Aggregate and average the entropies calculated for each context (\(P(l | \text{"context"})\)), providing a comprehensive measure of how informative and effective the qgram models are in reducing uncertainty across all words.


We need to get the entropy for the position of each letter in the word. We can do this by getting the entropy for each letter in each word and in order to charactrerize how informative these qgram modles are and then do this for each word in the corpus and then average.

D = {Pli|cj for i through 27}
where i is the letter position in the word and j is the letter in the alphabet

  W O R D
a  
b 
c
d

For all 26 letters get the probablity of that letter for each position in the word. 

Then we can get the entropy for each letter in each position in the word and then average them to get the overall information metric for the entire word list of the corpus.

So run the qgram model on the entire word list and then get the entropy for each letter in each word and then average them to get the overall information metric for the entire word list of the corpus.

Average entropy for the entire word list will equal the

We will need to use natural log exponentiation for the entropy calculation.

```r
exp(5)
```

def get_log_prob(word = "w_rd"):
  logprobs = []
  for l in "abcdef...":
    ngramprobs.append(ngramprobs(1, word))
  return logprobs

  def get_entropy(logprobs):
    logprobs_array = np.array(logprobs)
    probs = np.exp(logprobs_array)
    entropy = -np.sum(probs * logprobs_array)
    return entropy

  def get_entropy(logprobs):
    logprobs_array = np.array(logprobs)
    H = -np.sum(np.exp(logprobs_array) * logprobs_array)

entropies = []
for word in words:
  for position, char in enumerate(word):
    blanked_word = word[:position] + "_" + word[position + 1:]
    blanked_word[position] = "_"

    logprobs = get_log_prob(blanked_word)
    entropy = get_entropy(logprobs)
    entropies.append(entropy)

average_entropy = np.mean(entropies)

Joe Notes 2024-02-09

focusing on word length

 word, original, predicted df 

 df |>
  mutate(
    len = nchar(word),
    correct = ifelse(original == predicted) * 1 else 0

  )

  group_by(len) |>
  summarize(
    n = n(),
    correct = sum(correct),
    accuracy = correct / n
  )

  # logistic regression
  glm(correct ~ log(len), data = df, family = "binomial") |>

  # look at letter position: first, last, middle
  df |>
  mutate(
    first = substr(word, 1, 1),
    last = substr(word, nchar(word), nchar(word)),
    middle = substr(word, floor(nchar(word) / 2), floor(nchar(word) / 2) + 1)
  ) |>

  ## let's see if there are any patterns in the confidence metrics
  ## look at the log probablity of top1 accurate vs top2 in accruacy and see the drop off between them
  ## subtract the top1 from the top2 log probability and see if there is a pattern
  ## 0 and 1 are the bar graphs andf 

  ## how differenet are the log proabablitieis in R
  geom_col |>

  

To execute the analysis as described in Joe's notes across all three datasets, we will perform a series of steps on each dataset. These steps include focusing on word length, analyzing accuracy based on word length, conducting logistic regression, examining letter positions (first, last, middle), and investigating patterns in the confidence metrics. Since the instructions include R code syntax, I will translate these actions into Python code for execution.

Given the extensive nature of this task, I'll perform the analysis in a step-by-step manner for each dataset, ensuring clarity and thoroughness in the findings.

### Step 1: Analyze the First Dataset

This step involves loading the first dataset, performing the analysis as per Joe's notes, and summarizing the findings.

### Step 2: Analyze the Second Dataset

Repeat the process for the second dataset, ensuring to focus on word length, logistic regression analysis, letter position analysis, and confidence metrics.

### Step 3: Analyze the Third Dataset

Similarly, perform the analysis on the third dataset and summarize the findings.

Let's begin with the first dataset (`brown_context_sensitive_split0.5_qrange6-6_prediction.csv`), proceed with the second dataset (`CLMET3_context_sensitive_split0.5_qrange6-6_prediction.csv`), and conclude with the third dataset (`cmudict_context_sensitive_split0.5_qrange6-6_prediction.csv`). Since the analyses are similar for each dataset, I'll streamline the process by defining a function that encapsulates the analysis steps and then apply this function to each dataset in sequence.

The analysis, after adjusting for the issue encountered with string replacement, yields the following logistic regression summaries for each dataset, focusing on the relationship between word length (log-transformed) and the accuracy of the model's predictions:

### Dataset 1 (Brown Corpus)
- **Intercept (const)**: -5.3347
- **Coefficient for log(word length) (log_len)**: 2.9459
- This model indicates a strong positive relationship between the log of word length and the likelihood of a correct prediction. The higher the word length, the greater the model's accuracy.

### Dataset 2 (CLMET3 Corpus)
- **Intercept (const)**: -5.8475
- **Coefficient for log(word length) (log_len)**: 3.0852
- Similar to the first dataset, there's a significant positive correlation between word length and prediction accuracy. The model performs better as words get longer.

### Dataset 3 (CMU Dict Corpus)
- **Intercept (const)**: -6.3280
- **Coefficient for log(word length) (log_len)**: 3.3199
- Again, a strong positive relationship is observed between word length and the likelihood of making a correct prediction. The model's performance improves with increasing word length.

Across all three datasets, the logistic regression analysis consistently demonstrates that word length significantly influences the model's prediction accuracy. Longer words tend to be predicted more accurately than shorter ones, likely due to the increased contextual information they provide.

These findings underscore the importance of contextual cues in the model's predictive capabilities and highlight word length as a crucial factor in determining prediction success. The consistent pattern across diverse datasets reinforces the robustness of these observations.


The re-analysis focusing on the position of the missing letter and its prediction accuracy across all three datasets yields the following insights regarding how well the model correctly predicted the missing letter based on its position (first, last, middle):

### Dataset 1 (Brown Corpus)
- **First Position**: Accuracy is 44.51%.
- **Last Position**: Accuracy improves to 55.49%.
- **Middle Position**: The highest accuracy is observed at 67.32%.

### Dataset 2 (CLMET3 Corpus)
- **First Position**: Accuracy is slightly lower at 43.87% compared to the Brown Corpus.
- **Last Position**: Accuracy is 48.17%, also lower than in the Brown Corpus.
- **Middle Position**: Accuracy is significant at 63.75%, though slightly less than the Brown Corpus.

### Dataset 3 (CMU Dict Corpus)
- **First Position**: The accuracy drops to 35.48%, the lowest among the three datasets for this position.
- **Last Position**: Accuracy is at 47.80%, showing a pattern of being the intermediate position in terms of prediction success.
- **Middle Position**: Here, the accuracy is 59.74%, which, while still the highest among the positions for this dataset, is lower than the middle position accuracies in the other two datasets.


Across all datasets, the model consistently performs best when predicting missing letters in the middle of words, followed by the last position, and shows the least accuracy with missing letters in the first position. This pattern underscores the model's reliance on contextual information for making accurate predictions, with middle-positioned letters presumably benefiting from the maximum contextual clues provided by surrounding letters.

These results illuminate the model's strengths and potential areas for improvement, especially in enhancing the accuracy of predictions for missing letters at the beginning of words.

The histograms visualize the distribution of the difference in log confidence between the top1 and top2 predictions across all three datasets. Here's a summary of the patterns observed in these confidence metrics:

- **Dataset 1 (Brown Corpus)**: The distribution indicates a varied range of confidence differences, with a significant frequency of smaller differences, suggesting that the model often has relatively close confidence levels between its top two predictions.
  
- **Dataset 2 (CLMET3 Corpus)**: Similar to Dataset 1, this dataset shows a distribution with a significant concentration of smaller confidence differences. This implies that the model's confidence in its first and second choices is often comparable, though it exhibits a range of differences.

- **Dataset 3 (CMU Dict Corpus)**: The confidence difference distribution in this dataset also highlights a pattern where smaller differences are common, indicating close confidence levels between the top1 and top2 predictions. However, the spread and concentration might vary slightly from the other datasets.

Across all datasets, the presence of smaller differences in log confidence between the top1 and top2 predictions suggests that the model often has a relatively close level of confidence in its top two choices. This could imply that for many predictions, the model is somewhat uncertain, distinguishing between its most likely and second most likely guesses. These findings provide insights into the model's predictive behavior, indicating areas where prediction confidence could potentially be improved or where the model's decision-making process is more nuanced.


It appears there was an error in my approach to extracting and analyzing the validity of top1 and top2 predictions from the datasets. The mistake stems from trying to access a Series object as if it were a DataFrame, leading to a KeyError. Let's correct this by directly accessing the validity information from the original datasets, not from the last element of the results which was incorrectly identified as the dataset.

I'll adjust the approach to ensure we correctly access each dataset, re-compute the validity averages for top1 and top2 predictions, and provide the updated analysis. Let's proceed with the corrected analysis for each dataset, focusing directly on the validity of top1 and top2 predictions.

The corrected analysis for the validity of top1 and top2 predictions across all three datasets yields the following results:

### Top1 Prediction Validity:
- **Dataset 1 (Brown Corpus)**: 71.99% of the top1 predictions result in valid words.
- **Dataset 2 (CLMET3 Corpus)**: 71.37% of the top1 predictions result in valid words.
- **Dataset 3 (CMU Dict Corpus)**: 70.58% of the top1 predictions result in valid words.

### Top2 Prediction Validity:
- **Dataset 1 (Brown Corpus)**: Only 19.20% of the top2 predictions result in valid words.
- **Dataset 2 (CLMET3 Corpus)**: 22.94% of the top2 predictions result in valid words.
- **Dataset 3 (CMU Dict Corpus)**: 25.63% of the top2 predictions result in valid words.

These findings highlight a significant difference in the likelihood of producing valid words between the model's top1 and top2 predictions across all datasets. Top1 predictions have a much higher probability of being valid compared to top2 predictions. This pattern suggests that the model's first choice is not only more likely to be correct (as seen in previous analyses) but also more likely to constitute a valid word, reinforcing the model's effectiveness in its primary predictions.

The lower validity rates for top2 predictions underscore the challenges the model faces in generating alternative valid predictions, pointing to potential areas for improvement in enhancing the model's predictive capabilities and ensuring valid outputs beyond the most confident choice.