Task 1:
For the Split type B do an after the fact analysis seeing which words weren't in the trinaing set and see how well it did for predicting the missing letter in them. Split type B was the version of the code were the frequcny of each token was maintained in the testing and training but the out of vocabulary nature of the training and test split wasn't maintained. So both training and testing tokens can contain the same word types. So what we're looking for is just finding out of the tokens that did uniquely occur in the testing set how well did the model perform on them. 


Task 2:
Exclude hapax legomena (words occurring only once within the dataset) from the training dataset, and instead, train the model on the remaining data. Following training, evaluate the model's performance on the hapax legomena. This approach mirrors Task 1 but with a distinct separation: the training data excludes hapaxes, which are then used as the testing data. The goal is to determine the model's proficiency in predicting missing letters in words that are hapax legomena, thus providing insight into its ability to handle rare or unique words.

Task 3:

Goal is to measure the informational content of the entire word list by calculating the entropy of each letter in every word. This involves estimating the uncertainty associated with each letter's occurrence in different contexts, such as when a letter is missing from a word. The process can be broken down into several steps:

Start by assigning a probability to every alphabet letter based on how often it appears in the language. This initial step helps us estimate how likely each letter is to occur in any given position.

Refine these probabilities with qgram analysis, which considers the specific letters surrounding a missing letter to improve our prediction accuracy. This process narrows down the most likely letters by comparing broader frequencybased estimates to those informed by immediate letter context, thus aiming to reduce our prediction's entropy or uncertainty.

Calculate entropy in two ways:

First, using the letter frequencies (unigram probabilities) to see how much uncertainty there is without considering the surrounding letters.
Second, by assuming each letter is equally likely (uniform probability), representing the highest entropy scenario or the maximum uncertainty.
Assess the information gain by comparing the entropy from our unigram model to that of the uniform probability model. The difference tells us how much more accurate our predictions are when using the qgram model over a random guess.

Finally, average the entropy across all words for each letter in different contexts to determine the overall informativeness of the qgram models. This means calculating the entropy for each scenario where a letter is missing, then averaging these values to get a sense of how well the qgram models perform across the entire corpus.

In essence, we're evaluating how effective qgram models are by starting with a broad prediction based on letter frequency, refining it with contextspecific data\*\*, and then measuring the improvement in our predictions through entropy reduction. This process highlights the value of context in improving the accuracy of our language models.

We need to get the overall information metric for the entire word list of the corpus. We can do this by getting the entropy for each letter in each word and in order to charactrerize how informative these qgram modles are and then do this for each word in the corpus and then average.

The process can be broken down into several steps:

 1. Initial Probability Assignment
 Objective: Estimate the likelihood of each letter's occurrence across all positions within a word, based on its overall frequency in the language. This foundational step assigns a baseline probability, \(P(l)\), to each letter \(l\) in the alphabet.
 Methodology: Compute the frequency of each letter in a large corpus, normalizing these frequencies to probabilities. This calculation forms the basis for further refinement and adjustment.

 2. Refinement Using QGram Probabilities
 Objective: Enhance the predictive accuracy of initial probabilities by incorporating the context provided by qgrams, which are sequences of characters including the missing letter and its immediate neighbors.
 Methodology: Adjust letter probabilities (\(P(l)\)) to \(P(l | \text{"context"})\) by analyzing qgram frequencies within the corpus. This step leverages conditional probabilities to account for the influence of surrounding letters, significantly reducing prediction entropy.

 3. Entropy Calculation
 Objective: Measure the uncertainty or information content associated with the occurrence of each letter in a given context, employing two distinct entropy calculations to evaluate this uncertainty.
 Methodology:
   Unigram Entropy (\(H_{\text{unigram}}(l)\)): Calculated from the refined probabilities, indicating the entropy without specific context.
   Uniform Entropy (\(H_{\text{uniform}}\)): Assumes an equal probability for all letters, serving as a baseline for maximum uncertainty.

 4. Information Gain (Reduction in Entropy)
 Objective: Quantify the improvement in predictive accuracy by comparing the entropy from the refined model against the uniform model, thereby illustrating the effectiveness of qgram analysis.
 Methodology: Determine the information gain by calculating the difference between \(H_{\text{unigram}}(l)\) and \(H_{\text{uniform}}\), with significant reductions indicating a substantial increase in predictive power due to contextual considerations.

 5. Average Entropy Across Words
 Objective: Evaluate the overall performance and informativeness of qgram models across the corpus by calculating the average entropy for each letter in different contexts.
 Methodology: Aggregate and average the entropies calculated for each context (\(P(l | \text{"context"})\)), providing a comprehensive measure of how informative and effective the qgram models are in reducing uncertainty across all words.
