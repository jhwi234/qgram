To understand the math behind going from a corpus token size and an assumed alpha (typically alpha = 1 for an ideal Zipfian distribution) to calculating the vocabulary size (unique words/word types), we need to delve into the principles of Zipf's Law and its mathematical representation.

### Zipf's Law:

Zipf's Law is an empirical law that states that the frequency of any word in a corpus is inversely proportional to its rank in the frequency table. Mathematically, it can be represented as:

\[ f(r) = \frac{C}{r^\alpha} \]

where:
- \( f(r) \) is the frequency of the word at rank \( r \).
- \( C \) is a constant approximately equal to the frequency of the most common word.
- \( \alpha \) is an exponent usually close to 1 (in an ideal Zipfian distribution, \( \alpha = 1 \)).

### Generalized Harmonic Number:

The generalized harmonic number is used to normalize the frequencies so that their sum equals the total number of words in the corpus. It is defined as:

\[ H(N, \alpha) = \sum_{i=1}^{N} \frac{1}{i^\alpha} \]

where \( N \) is the number of terms in the series (i.e., the vocabulary size).

### Calculating Vocabulary Size:

Given a corpus with a total token size \( T \) and assuming an ideal Zipfian distribution (where \( \alpha = 1 \)), the goal is to find the vocabulary size \( N \) such that the sum of the frequencies of all words equals \( T \). The frequency of a word at rank \( r \) in an ideal Zipfian distribution (with \( \alpha = 1 \)) can be expressed as:

\[ f(r) = \frac{T}{H(N, 1)} \cdot \frac{1}{r} \]

The total number of tokens can be obtained by summing the frequencies of all words:

\[ T = \sum_{r=1}^{N} f(r) = \frac{T}{H(N, 1)} \sum_{r=1}^{N} \frac{1}{r} \]

Since \( \sum_{r=1}^{N} \frac{1}{r} \) is \( H(N, 1) \), it simplifies to:

\[ T = \frac{T}{H(N, 1)} \cdot H(N, 1) \]

Therefore, to find the vocabulary size \( N \), we need to find the largest \( N \) such that \( H(N, 1) \) (the harmonic number) when multiplied by the frequency of the most common word (approximately \( \frac{T}{H(N, 1)} \)) sums up to the total number of tokens \( T \). 

This calculation often requires iterative methods like binary search, as there's no direct algebraic way to solve for \( N \) from \( H(N, 1) \) and \( T \). The binary search iteratively guesses values of \( N \), recalculates \( H(N, 1) \), and checks if the total frequency matches \( T \). The correct vocabulary size \( N \) is the one that makes this equation true.

Calculating the vocabulary size (the number of unique words or word types) from a given corpus token size, assuming an ideal Zipfian distribution with a specific alpha (e.g., alpha = 1), involves understanding the properties of the Zipfian distribution and some mathematical reasoning.

### Zipf's Law:

Zipf's Law states that the frequency of a word in natural language is inversely proportional to its rank in the frequency table. Mathematically, the frequency \( f \) of the \( n \)-th most common word is proportional to \( \frac{1}{n^\alpha} \), where \( \alpha \) is typically close to 1 in natural languages.

### Generalized Harmonic Number:

The generalized harmonic number is used to sum the frequencies of words in a Zipfian distribution. For a vocabulary of size \( V \) and an alpha \( \alpha \), the generalized harmonic number \( H(V, \alpha) \) is given by:

\[ H(V, \alpha) = \sum_{i=1}^{V} \frac{1}{i^\alpha} \]

This sum helps in normalizing the frequencies so that they add up to the total size of the corpus.

### Finding the Vocabulary Size:

The goal is to find the number of unique words \( V \) such that the sum of their frequencies equals the total number of words (corpus token size) in the corpus. 

Given a corpus with \( N \) total words (corpus token size) and assuming an ideal Zipfian distribution, the expected frequency of the \( i \)-th word is:

\[ f_i = \frac{N}{H(V, \alpha)} \cdot \frac{1}{i^\alpha} \]

The total number of words can then be expressed as:

\[ N = \sum_{i=1}^{V} f_i = \frac{N}{H(V, \alpha)} \cdot \sum_{i=1}^{V} \frac{1}{i^\alpha} \]

Since the right-hand side also equals \( H(V, \alpha) \), we get:

\[ N = \frac{N}{H(V, \alpha)} \cdot H(V, \alpha) \]

The challenge lies in solving this equation for \( V \) given \( N \) and \( \alpha \), which is not straightforward due to the nature of the generalized harmonic number.

### Yule's K Measure:

Yule's K is a measure of lexical diversity, specifically focusing on word frequency distribution. It's calculated using the frequencies and squared frequencies of words in a corpus. The formula for Yule's K is as follows:

\[
K = 10^4 \left( \frac{\sum_{i=1}^{V} f_i^2 - N}{N^2} \right)
\]

Where:
- \( f_i \) is the frequency of the \( i^{th} \) word.
- \( V \) is the number of unique words (vocabulary size).
- \( N \) is the total number of words in the corpus.
- The summation \( \sum_{i=1}^{V} f_i^2 \) calculates the sum of the squared frequencies of each word.

### Heaps' Law (K and Beta):

Heaps' Law predicts the growth of vocabulary size as a function of the size of the corpus. The relationship is often described by the following formula:

\[
V(R) = K \cdot R^\beta
\]

Where:
- \( V(R) \) is the vocabulary size (number of unique words) when the corpus size is \( R \) words.
- \( K \) is a constant representing the rate of vocabulary growth at the start of the corpus.
- \( \beta \) is a parameter that indicates the rate at which new words appear as the corpus size increases.
- \( R \) is the size of the corpus (total number of words).

In practice, to find the parameters \( K \) and \( \beta \), a log-log plot is often used where the logarithm of the vocabulary size is plotted against the logarithm of the corpus size. The slope of the line in this plot gives \( \beta \), and the intercept gives \( \log(K) \).

### Interpretation and Application:

- **Yule's K** is typically used to measure the concentration of repetition and lexical diversity within a single text or corpus.
- **Heaps' Law** is applied to predict how vocabulary size will grow as more text is added to a corpus. It's often used in computational linguistics for tasks like estimating database size for indexing or predicting the complexity of language models.

The Zipf-Mandelbrot law is a refined version of Zipf's law, designed to better accommodate the distribution of ranked data, particularly in linguistic contexts. It's a discrete probability distribution that provides a more flexible model for describing the frequency of words in text corpora, ecological data, and other ranked phenomena.

### Equation and Mathematical Formulation:

The Zipf-Mandelbrot law is defined by the following probability mass function (PMF):

\[ f(k; N, q, s) = \frac{1}{(k + q)^s \cdot H_{N,q,s}} \]

where:
- \( k \) is the rank of an element.
- \( N \) is the total number of elements.
- \( q \) and \( s \) are parameters that modify the distribution.
- \( H_{N,q,s} \) is a normalizing constant, given by the generalized harmonic number:

  \[ H_{N,q,s} = \sum_{i=1}^{N} \frac{1}{(i + q)^s} \]

As \( N \) approaches infinity, \( H_{N,q,s} \) can be approximated by the Hurwitz zeta function \( \zeta(s, q) \).

### Key Mathematical Properties:

1. **Generalization**: The Zipf-Mandelbrot law generalizes Zipf's law by introducing an additional parameter \( q \), which shifts the rank. This parameter allows for more flexibility in modeling real-world data where the simplest Zipfian model may not fit well.

2. **Reduction to Zipf's Law**: When \( q = 0 \) and \( N \) is finite, the Zipf-Mandelbrot law simplifies to Zipf's law. For infinite \( N \) and \( q = 0 \), it aligns with the Zeta distribution.

3. **Harmonic Number**: The normalizing constant \( H_{N,q,s} \) is a crucial component that ensures the PMF sums to 1 across all ranks. It can be computationally intensive to calculate for large \( N \), and approximations are often used in practical applications.

4. **Parameter Sensitivity**: The behavior of the distribution is sensitive to changes in \( s \) and \( q \). The parameter \( s \) primarily controls the steepness of the distribution, while \( q \) adjusts the distribution's shift along the rank axis.

### Understanding the Parameters:

- **The parameter \( s \)**: This parameter influences the slope of the frequency-rank relationship. A larger \( s \) generally results in a steeper decay of frequency with rank, indicating a more pronounced disparity between the frequencies of high-ranked and low-ranked items.
  
- **The parameter \( q \)**: The shift parameter \( q \) modifies how the ranking starts. A higher \( q \) value effectively shifts the frequency curve to the right, changing the distribution's emphasis on higher-ranked items.

### Applications:

- **Linguistics**: In text corpora, the law models the frequency of word occurrences. Different values of \( s \) and \( q \) can reflect variations in language usage, genre, or author style.
- **Ecology**: Used to model the relative abundance of species, reflecting the ecological diversity and distribution of species in a habitat.
- **Music**: The distribution of musical notes and their relative frequencies in compositions may conform to the Zipf-Mandelbrot law.

### Computational Considerations:

In practical applications, especially in computational linguistics and data science, it's crucial to efficiently compute the normalizing constant \( H_{N,q,s} \) and to accurately estimate the parameters \( s \) and \( q \) from empirical data. Curve fitting techniques, often involving logarithmic transformations and linear regression, are commonly employed to estimate these parameters. The challenge lies in balancing computational feasibility with the precision of the model, especially when dealing with large datasets.

The paper by Chakrabarty et al. details a comprehensive methodology for estimating the parameters of the Zipf—Mandelbrot law in the context of text analysis. Here is an expanded explanation of their approach:

### Model Assumptions:
1. **Infinite Dictionary**: The model assumes an infinitely large dictionary. Words in a text are considered to be selected independently from this dictionary.
2. **Probability Distribution**: The probability \( p_i \) of the occurrence of a word based on its rank \( i \) is governed by the Zipf—Mandelbrot distribution, formulated as:
   \[ p_i = \frac{c}{(i + q)^{\alpha}} \]
   where \( c \) is a normalization constant ensuring the sum of probabilities equals 1, \( \alpha = \frac{1}{\theta} \) is a parameter influencing the frequency of word occurrences, and \( q > -1 \) is the Mandelbrot shift parameter, introduced to enhance empirical data fitting.

### Estimating Parameters - θ and q:
1. **Estimator for θ (θb):**
   - The estimator \( \theta_b \) is defined by the integral:
     \[ \theta_b = \int_0^1 \log^+ R[nt] dA(t) \]
   - Here, \( R[k] \) represents the number of distinct words among the first \( k \) words of the text.
   - The notation \( \log^+ x \) implies the positive part of the logarithm, meaning \( \log^+ x = \max(\log x, 0) \).
   - Function \( A(\cdot) \) is chosen such that \( \int_0^1 \log t dA(t) = 1 \) and \( A(0) = A(+0) = A(1) = 0 \).
   - A specific example for \( A(t) \) provided in the paper simplifies \( \theta_b \) to \( \log_2(R[n]/R[n/2]) \), where \( R[n] \) is the number of distinct words in the entire text.

2. **Finding q (qb):**
   - The estimation of \( q \) involves adjusting \( q_b \) to match the estimated number of distinct words \( r(n) \) with the actual observed count \( R[n] \).
   - The estimator \( r(k) \) for distinct word counts is computed based on the estimated values of \( p_i \) and \( q_b \).
   - \( q_b \) is iteratively adjusted until \( r(n) \) aligns with \( R[n] \).

### Process of the Numbers of Different Words (R1, ..., Rn):
- The sequence \( R1, ..., Rn \) is pivotal in the methodology. It represents the count of unique words identified upon reading through the first \( n \) words of a text.
- This sequential count is crucial for the estimation of \( \theta \) and \( q \), as well as for the construction of the statistical test to verify the model’s appropriateness.

### Statistical Test Development:
- The authors develop a statistical test to determine if a text conforms to the Zipf—Mandelbrot law.
- This involves constructing the 'empirical text bridge', a stochastic process derived from the sequence \( R[k] \) and the parameter estimates.
- The behavior of this empirical bridge, particularly its convergence to a Gaussian process, is examined to evaluate how well a text fits the Zipf—Mandelbrot law.

In summary, the paper provides a detailed, statistically rigorous approach to estimating the parameters of the Zipf—Mandelbrot law and testing its applicability to natural language texts. This methodology not only quantifies word frequency distributions but also assesses the extent to which these distributions follow theoretical expectations.